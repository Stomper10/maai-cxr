{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c98399f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer 'densenet_stem_conv' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]\n\nCall arguments received by layer 'densenet_stem_conv' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 326, 326, 1), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mLOAD FULL MODEL (TF)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m A2IModel(configs\u001b[38;5;241m=\u001b[39mconfigs)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# ===== CHECK YOUR FULL MODEL WEIGHT DIRECTORY ===== \u001b[39;00m\n\u001b[1;32m     63\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(full_model_weight_path)\n",
      "File \u001b[0;32m~/workspace/maai-cxr/src/kslee001/xai/modules/model.py:402\u001b[0m, in \u001b[0;36mA2IModel.initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m img_sample \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mimage_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mimage_channels))\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# aux_sample = tf.zeros((1,2))\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/workspace/maai-cxr/src/kslee001/xai/modules/model.py:204\u001b[0m, in \u001b[0;36mA2IModel.call\u001b[0;34m(self, X, training)\u001b[0m\n\u001b[1;32m    202\u001b[0m x \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"image forward\"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"label classification features\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m atel_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassify(feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matel_classifier, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/workspace/maai-cxr/src/kslee001/xai/modules/model.py:173\u001b[0m, in \u001b[0;36mA2IModel.extract_feature\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m    167\u001b[0m             feature_for_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;66;03m# if ('densenet_stem_zeropad1' in layer_name):\u001b[39;00m\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;66;03m#     print(feature.shape)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;66;03m#     feature = getattr(self, layer_name)(feature, training=training)     \u001b[39;00m\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m             feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvnext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, layer_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor):\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling layer 'densenet_stem_conv' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} DNN library is not found. [Op:Conv2D]\n\nCall arguments received by layer 'densenet_stem_conv' (type Conv2D):\n  • inputs=tf.Tensor(shape=(1, 326, 326, 1), dtype=float32)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import shap\n",
    "#import shap.explainers.deep.deep_tf\n",
    "import lime\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "import ssl\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "from modules.model import A2IModel\n",
    "# from modules_label_ensemble.model import Expert as A2IModel\n",
    "from modules.lr_scheduler import CustomOneCycleSchedule, LearningRateLogger\n",
    "import functions\n",
    "from cfg import configs\n",
    "# from cfg_label_ensemble import configs\n",
    "\n",
    "\n",
    "data_path = '/home/gyuseonglee/workspace/dataset/chexpert-resized'\n",
    "# data_path = YOUR_DATA_DIR\n",
    "full_model_weight_path = '/home/gyuseonglee/workspace/maai-cxr/src/kslee001/quantization/weights/ensemble/densenet121_7000_test.h5'\n",
    "# full_model_weight_path = '/home/gyuseonglee/workspace/maai-cxr/src/kslee001/quantization/weights/label_ensemble/atel/atel_densenet121_317_test.h5'\n",
    "# quant_model_wieght_path = '/home/gyuseonglee/workspace/maai-cxr/src/kslee001/ensemble/densenet121_1005.tflite'\n",
    "# path = YOUR_MODEL_PATH\n",
    "\n",
    "configs.model.backbone = 'densenet'\n",
    "configs.wandb.use_wandb = False\n",
    "configs.general.batch_size = 1\n",
    "\n",
    "# ===== CHECK YOUR DATA DIRECTORY ===== \n",
    "configs.dataset.data_dir = data_path\n",
    "# ===== CHECK YOUR DATA DIRECTORY ===== \n",
    "\n",
    "configs.dataset.cutoff = 1000\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = functions.load_datasets(configs) \n",
    "configs.general.steps_per_epoch = train_dataset.steps_per_epoch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "LOAD FULL MODEL (TF)\n",
    "\"\"\"\n",
    "model = A2IModel(configs=configs)\n",
    "model.initialize()\n",
    "\n",
    "# ===== CHECK YOUR FULL MODEL WEIGHT DIRECTORY ===== \n",
    "model.load_weights(full_model_weight_path)\n",
    "# ===== CHECK YOUR FULL MODEL WEIGHT DIRECTORY ===== \n",
    "\n",
    "\n",
    "criterion = tf.keras.losses.CategoricalCrossentropy(\n",
    "    # from_logits=True,\n",
    "    from_logits=False, \n",
    "    label_smoothing=configs.model.label_smoothing,\n",
    "    reduction=tf.keras.losses.Reduction.SUM if configs.general.distributed else 'auto'\n",
    ")\n",
    "model.compile(loss=criterion)\n",
    "model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "#import shap.explainers.deep.deep_tf\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.lime_image import LimeImageExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8af75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(valid_dataset))\n",
    "image = batch[0][0][:, :, 0]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image):\n",
    "    return model(image)[0] # atel prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(batch[0]).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9ec744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor shape :  (1, 320, 320, 3)\n",
      "numpy  shape  (320, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "# get positive sample\n",
    "a = 0\n",
    "for idx, batch in enumerate(valid_dataset):\n",
    "    if (batch[1][0][0] == 1.0):\n",
    "        if classify(batch[0]).numpy()[0] > 0.5:\n",
    "            break\n",
    "        \n",
    "# image, label = next(iter(valid_dataset))\n",
    "image, label = batch\n",
    "image = tf.concat([image, image, image], axis=-1)\n",
    "image_numpy = image.numpy().squeeze()\n",
    "label = label[:].numpy()\n",
    "print(\"tensor shape : \", image.shape)\n",
    "print(\"numpy  shape \",image_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d78d089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5838333 , 0.41616672], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = classify(image)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eada649c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4990ab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8003b5cd007040068fbd967d261fa577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime_image\u001b[38;5;241m.\u001b[39mLimeImageExplainer()\n\u001b[0;32m----> 2\u001b[0m explainer_l \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_numpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhide_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda/envs/tf/lib/python3.9/site-packages/lime/lime_image.py:216\u001b[0m, in \u001b[0;36mLimeImageExplainer.explain_instance\u001b[0;34m(self, image, classifier_fn, labels, hide_color, top_labels, num_features, num_samples, batch_size, segmentation_fn, distance_metric, model_regressor, random_seed)\u001b[0m\n\u001b[1;32m    212\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mtop_labels\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m top:\n\u001b[1;32m    214\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[1;32m    215\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[0;32m--> 216\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance_with_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_regressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret_exp\n",
      "File \u001b[0;32m~/anaconda/envs/tf/lib/python3.9/site-packages/lime/lime_base.py:182\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Takes perturbed data, labels and distances, returns explanation.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    local_pred is the prediction of the explanation model on the original instance\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[0;32m--> 182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m \u001b[43mneighborhood_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(neighborhood_data,\n\u001b[1;32m    184\u001b[0m                                        labels_column,\n\u001b[1;32m    185\u001b[0m                                        weights,\n\u001b[1;32m    186\u001b[0m                                        num_features,\n\u001b[1;32m    187\u001b[0m                                        feature_selection)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explainer_l = explainer.explain_instance(\n",
    "    image=image_numpy,\n",
    "    labels=label,\n",
    "    classifier_fn=classify, \n",
    "    hide_color=0, \n",
    "    top_labels=1, \n",
    "    num_samples=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f85fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(explainer_l.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e176754",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explainer_l.get_image_and_mask(1, positive_only=False, num_features=40, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a08357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_sample(exp, exp_class, weight = 0.1, show_positive = False, hide_background = False):\n",
    "    '''\n",
    "    Method to display and highlight super-pixels used by the black-box model to make predictions\n",
    "    '''\n",
    "    image, mask = exp.get_image_and_mask(exp_class, \n",
    "                                         positive_only=show_positive, \n",
    "                                         num_features=6, \n",
    "                                         hide_rest=hide_background,\n",
    "                                         min_weight=weight\n",
    "                                        )\n",
    "    plt.imshow(mark_boundaries(image, mask))\n",
    "    #plt.savefig('lime_pred.png')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08988ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prediction_sample(explainer_l, explainer_l.top_labels[1], show_positive = True, hide_background = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explainer_l.get_image_and_mask(\n",
    "    240, positive_only=True, num_features=2, hide_rest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a75d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_l = explainer.explain_instance(images[0], new_model.predict, hide_color=0, top_labels=2, num_samples=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c11f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f20b8a6",
   "metadata": {},
   "source": [
    "### shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get positive sample\n",
    "a = 0\n",
    "for idx, batch in enumerate(valid_dataset):\n",
    "    if (batch[1][0][0] == 1.0):\n",
    "        if classify(batch[0]).numpy()[0][1] > 0.5:\n",
    "            break\n",
    "        \n",
    "# image, label = next(iter(valid_dataset))\n",
    "image, label = batch\n",
    "image_numpy = tf.concat([image, image, image], axis=-1)\n",
    "image_numpy = image.numpy().squeeze()\n",
    "label = label[:, 0].numpy()\n",
    "print(\"tensor shape : \", image.shape)\n",
    "print(\"numpy  shape \",image_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac47d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed3cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_images = []\n",
    "for idx, batch in enumerate(valid_dataset):\n",
    "#     x = tf.squeeze(batch[0], axis=0)\n",
    "    valid_images.append(tf.reshape(tf.concat([x,x,x], axis=-1), [320, 320, 3]))\n",
    "valid_images = np.stack(valid_images)\n",
    "valid_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec5e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "masker0 = shap.maskers.Image(\"inpaint_telea\", image.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background = valid_images[np.random.choice(valid_images[0].shape, 50, replace=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddee6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(image)[0].numpy()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3d809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_wrapper(x):\n",
    "    out = model(x)\n",
    "    return out[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.DeepExplainer(model_wrapper, valid_images)\n",
    "# shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3cb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57646b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255f50a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae64a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e31a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# load package\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import shap\n",
    "#import shap.explainers.deep.deep_tf\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.lime_image import LimeImageExplainer\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "import ssl\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ensemble.modules.model import A2IModel\n",
    "from models.ensemble.modules.lr_scheduler import CustomOneCycleSchedule, LearningRateLogger\n",
    "import models.ensemble.functions as functions\n",
    "from models.ensemble.cfg import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608de7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeplift\n",
    "from deeplift.layers import NonlinearMxtsMode\n",
    "from deeplift.conversion import kerasapi_conversion as kc\n",
    "from deeplift.util import compile_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "NUM_CLASSES = len(TARGET_COLUMNS)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMAGE_SIZE = [320, 320]\n",
    "BATCH_SIZE = 1\n",
    "MODEL_DIR = '/home/n0/a2i006/xai/models/ensemble/test/densenet121_10000_test.h5'\n",
    "MODEL_DIR_Q = '/home/n0/a2i006/xai/models/baseline_quant_model.tflite'\n",
    "\n",
    "DATASET_DIR = '/home/n0/a2i006/xai/dataset'\n",
    "DATASET_NAME = 'CheXpert-v1.0-small'\n",
    "TEST_SIZE = 0.1 # enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return : dataframe with columns [Path, label_1, label_2, ... label_n]\n",
    "def csv_to_df(dataset_dir, dataset_name):\n",
    "    # TODO : load data into pandas dataframe\n",
    "    data = pd.read_csv(f\"{dataset_dir}/valid.csv\").fillna(0.0)\n",
    "\n",
    "    # TODO : fix image path:\n",
    "    # ex. CheXpert-v1.0/patient0000...\n",
    "    # ex.       > /mnt/e/dataset/chexpert/CheXpert-v1.0/patient0000...\n",
    "    data['Path'] = data['Path'].str.replace(dataset_name, dataset_dir, regex=False)\n",
    "\n",
    "    # TODO : fill null values\n",
    "    for col_idx in range(5, len(data.columns)):\n",
    "        data[data.columns[col_idx]] = data[data.columns[col_idx]].astype(str)\n",
    "        data[data.columns[col_idx]] = data[data.columns[col_idx]].str.replace(\"-1.0\", \"0.0\").astype(float).fillna(0.0)\n",
    "\n",
    "    # TODO : column reduction\n",
    "    target_columns = ['Path'] + TARGET_COLUMNS\n",
    "    dataframe = data[target_columns].reset_index(drop=True)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326138d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path_validation(image_path, label):\n",
    "    # Read the image from the path\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1) #3\n",
    "    image = tf.image.resize(image, [320, 320])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434182c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset_dir):\n",
    "    # make train_images\n",
    "    train_images = []\n",
    "    for i in range(64541, 64741):\n",
    "        img_path = f\"{dataset_dir}/valid/patient{i}/study1/view1_frontal.jpg\"\n",
    "        img = image.load_img(img_path, target_size=(320, 320))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = x / 255.0\n",
    "        train_images.append(x)\n",
    "\n",
    "    # Convert the list to a numpy array\n",
    "    # (number_of_images, height, width, channels). (48, 320, 320, 3).\n",
    "    train_images = np.concatenate(train_images, axis=0) \n",
    "    #print(train_images.shape)\n",
    "    \n",
    "    return train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81807d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(data):\n",
    "    model_prediction = model.predict(data)\n",
    "    print(f\"The predicted class is : {decode_predictions(model_prediction, top=1)[0][0][1]}\")\n",
    "    return decode_predictions(model_prediction, top=1)[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf0ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_sample(exp, exp_class, weight = 0.1, show_positive = False, hide_background = False):\n",
    "    '''\n",
    "    Method to display and highlight super-pixels used by the black-box model to make predictions\n",
    "    '''\n",
    "    image, mask = exp.get_image_and_mask(exp_class, \n",
    "                                         positive_only=show_positive, \n",
    "                                         num_features=6, \n",
    "                                         hide_rest=hide_background,\n",
    "                                         min_weight=weight\n",
    "                                        )\n",
    "    plt.imshow(mark_boundaries(image, mask))\n",
    "    #plt.savefig('lime_pred.png')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([new_model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    #feed_dict = dict(zip([interpreter.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(new_model.layers[layer].input, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ac172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setting\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print('Num_GPUs:{}, List:{}'.format(len(physical_devices), physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from csv\n",
    "df = csv_to_df(DATASET_DIR, DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter filenames ending with \"_frontal.jpg\"\n",
    "valid_df = df[df['Path'].str.endswith('_frontal.jpg')]\n",
    "#print(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83564f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for validation\n",
    "list_ds_valid = tf.data.Dataset.from_tensor_slices((valid_df['Path'].values, valid_df.iloc[:, 1:].values))\n",
    "#print(list_ds_valid)\n",
    "ds_valid = list_ds_valid.map(process_path_validation, num_parallel_calls=AUTOTUNE)\n",
    "ds_valid = ds_valid.batch(1)\n",
    "#ds_valid = ds_valid.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(ds_valid))\n",
    "sample[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947d951f",
   "metadata": {},
   "source": [
    "Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = A2IModel(configs=configs)\n",
    "new_model.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dbfe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_model.load_weights(filepath = MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a61c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed353fc",
   "metadata": {},
   "source": [
    "Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e7bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "#model = load_model(MODEL_DIR, custom_objects={'CosineDecayWithWarmup': CosineDecayWithWarmup, 'AUROC': AUROC})\n",
    "interpreter = tf.lite.Interpreter(model_path=MODEL_DIR_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f116e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed76906",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_index = interpreter.get_input_details()[0]['index']\n",
    "input_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_index = interpreter.get_output_details()[0]['index']\n",
    "output_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7dd57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sum_correct = 0.0\n",
    "for idx, (x, y) in enumerate(ds_valid):\n",
    "    img = (x+1.0)/2.0*255.0\n",
    "    #image = tf.expand_dims(img, axis=0)\n",
    "    img = tf.cast(img, tf.uint8)\n",
    "    #print(image)\n",
    "    #break\n",
    "    interpreter.set_tensor(input_index, img)\n",
    "    interpreter.invoke()\n",
    "    pred = interpreter.get_tensor(output_index)\n",
    "    if np.argmax(pred) == np.argmax(y):\n",
    "        sum_correct += 1.0\n",
    "    \n",
    "    #if idx == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = sum_correct/float(idx+1)\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a797e46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cff40",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the validation dataset\n",
    "predictions = new_model.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc3ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atel_labels = (np.argmax(predictions[0], axis=1)).astype(int)\n",
    "atel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aaa83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plef_labels = (np.argmax(predictions[4], axis=1)).astype(int)\n",
    "plef_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b41f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 label==1일 확률\n",
    "probs = []\n",
    "for label_arr in predictions:\n",
    "    label_prob = []\n",
    "    for pred in label_arr:\n",
    "        prob = pred[1]\n",
    "        label_prob.append(prob)\n",
    "    probs.append(label_prob)\n",
    "predictions2 = np.empty((202,5))\n",
    "for i in range(len(probs[0])):\n",
    "    row = [elem[i] for elem in probs]\n",
    "    predictions2[i] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a98190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true labels from the validation dataset\n",
    "true_labels = np.array([label.numpy() for _, label in ds_valid])\n",
    "true_labels = np.reshape(true_labels, (-1, NUM_CLASSES))  # Reshape to 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = transform(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7031610",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a98c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explainer_l = explainer.explain_instance(images[0], new_model.predict, hide_color=0, top_labels=2, num_samples=1000)\n",
    "#explainer_l = explainer.explain_instance(images[0], interpreter.get_tensor, hide_color=0, top_labels=2, num_samples=1000)\n",
    "\n",
    "plt.imshow(explainer_l.segments)\n",
    "#plt.savefig('lime.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explainer.get_image_and_mask(240, positive_only=True, num_features=5, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_prediction_sample(exp, exp.top_labels[0], show_positive = True, hide_background = True)\n",
    "generate_prediction_sample(explainer_l, explainer_l.top_labels[1], show_positive = False, hide_background = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ffcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store dataset elements\n",
    "elements = []\n",
    "\n",
    "# Iterate over the dataset and collect elements\n",
    "for element in ds_valid:\n",
    "    resized_image = tf.image.resize(element[0].numpy(), IMAGE_SIZE)\n",
    "    print(element[1])\n",
    "    elements.append(resized_image.numpy())\n",
    "\n",
    "# Convert the elements list to a NumPy array\n",
    "elements = np.array(elements)\n",
    "# Reshape the elements array\n",
    "X = np.squeeze(elements, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in ds_valid:\n",
    "    label.append(i[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a masker that is used to mask out partitions of the input image, this one uses a blurred background\n",
    "masker0 = shap.maskers.Image(\"inpaint_telea\", X[0].shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526dd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[201].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    tmp = X.copy()    \n",
    "    tmp = tmp / 255.0\n",
    "    return new_model(tmp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea212ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X):\n",
    "    tmp = X.copy()    \n",
    "    tmp = tmp / 255.0\n",
    "    return new_model(tmp)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55db8600",
   "metadata": {},
   "source": [
    "Deep explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb806cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select backgroud for shap\n",
    "# training set\n",
    "background = images[np.random.choice(images.shape[0], 100, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84d5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8b456",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dea82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(masker0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ffdf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DeepExplainer to explain predictions of the model\n",
    "# model (output (,1)), data (numpy.array, pd.DataFrame)\n",
    "explainer_sd = shap.DeepExplainer(new_model, background[0])\n",
    "# X (list - numpy.array, pd.DataFrame)\n",
    "shap_values = explainer_sd(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8560dc",
   "metadata": {},
   "source": [
    "Vanilla explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ed773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer_s = shap.Explainer(f, masker0, output_names=TARGET_COLUMNS)\n",
    "# compute shap values\n",
    "#shap_values = explainer_s.shap_values(images[0])\n",
    "shap_values = explainer_s(X[:10], max_evals = 100, batch_size=BATCH_SIZE, outputs=shap.Explanation.argsort.flip[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    shap.image_plot(shap_values[i])\n",
    "    print(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6504c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    shap.image_plot(shap_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e957789",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    shap.image_plot(shap_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffacfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "f = h5py.File(MODEL_DIR)\n",
    "revealcancel_model = kc.convert_model_from_saved_files(\n",
    "                            h5_file=MODEL_DIR,\n",
    "                            nonlinear_mxts_mode=NonlinearMxtsMode.RevealCancel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6be106",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.get_layers()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
