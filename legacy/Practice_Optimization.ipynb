{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Lite model optimization\n",
    "\n",
    "TensorFlow Lite에서는 edge device에서 모델의 inference를 효율적으로 하기 위한 방법인 quantization과 pruning을 이용할 수 있는 toolkit을 제공하고 있습니다. 이번 실습에서는 해당 toolkit을 이용해 볼 것입니다.\n",
    "\n",
    "더 자세한 내용은 document를 참고 해주세요.\n",
    "- https://www.tensorflow.org/model_optimization/guide\n",
    "- https://www.tensorflow.org/lite/performance/model_optimization\n",
    "- TFLite https://www.tensorflow.org/lite/guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. What is TFLite?\n",
    "\n",
    "TensorFlow Lite는 TensorFlow로 학습된 모델을 tflite이라는 포맷으로 바꿔 줌으로써 모바일 장치, 임베디드 장치 등의 가벼운 장치에서 모델을 inference 할 수 있게 해주는 플랫폼 이다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/115159215-fccd0800-a0cc-11eb-817b-645f96c76966.png\" width=\"400\" height=\"400\"/>  \n",
    "\n",
    "\n",
    "TensorFlow는 모델을 파일로 저장할 때는 [Protocol Buffer](https://developers.google.com/protocol-buffers)라는 포맷으로 저장을 하고, TensorFlow Lite의 모델은 [FlatBuffer](https://google.github.io/flatbuffers/)라는 포맷으로 저장을 합니다. 두 포맷의 가장 큰 차이점은 FlatBuffer는 serialized 파일을 추가적인 메모리 할당 없이 deserialize를 할 수 있기 때문에 빠르게 파일에 있는 데이터에 접근할 수 있다.\n",
    "\n",
    "\n",
    "TFLite 모델로 변환을 할 때, quantization과 pruning 등의 optimization을 할 수 있는데, 이 때의 장점은 다음과 같다.\n",
    "\n",
    "- Size reduction\n",
    "  - smaller storage size, smaler download size, less memory usage\n",
    "- Latency reduction\n",
    "  - computation을 줄여 줌으로써 inference에 걸리는 시간 감소\n",
    "- Accelerator compatibility\n",
    "  - Edge TPU와 같이 특정 hardware는 각각의 quantization 요구 사항을 만족해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow 최신 버전 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.1.0\n",
      "Uninstalling tensorflow-2.1.0:\n",
      "  Successfully uninstalled tensorflow-2.1.0\n",
      "Collecting tf-nightly\n",
      "  Downloading tf_nightly-2.6.0.dev20210430-cp37-cp37m-manylinux2010_x86_64.whl (456.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 456.8 MB 56 kB/s s eta 0:00:01   |▋                               | 9.4 MB 10.6 MB/s eta 0:00:43     |▊                               | 10.9 MB 10.6 MB/s eta 0:00:42     |█▏                              | 16.3 MB 10.6 MB/s eta 0:00:42     |███▏                            | 45.8 MB 21.7 MB/s eta 0:00:19     |███▉                            | 55.0 MB 21.7 MB/s eta 0:00:19     |██████▏                         | 88.7 MB 8.6 MB/s eta 0:00:44     |██████▎                         | 89.6 MB 8.6 MB/s eta 0:00:43     |███████▎                        | 104.3 MB 27.2 MB/s eta 0:00:13     |█████████▎                      | 132.8 MB 13.8 MB/s eta 0:00:24     |████████████▍                   | 177.5 MB 20.5 MB/s eta 0:00:14     |██████████████                  | 200.4 MB 18.5 MB/s eta 0:00:14     |██████████████▎                 | 204.4 MB 18.5 MB/s eta 0:00:14     |███████████████▏                | 216.9 MB 17.1 MB/s eta 0:00:15     |█████████████████▎              | 246.5 MB 17.1 MB/s eta 0:00:13     |█████████████████▌              | 250.5 MB 17.1 MB/s eta 0:00:13     |███████████████████             | 272.0 MB 21.8 MB/s eta 0:00:09     |███████████████████▍            | 277.2 MB 21.8 MB/s eta 0:00:09     |██████████████████████████▏     | 373.1 MB 19.4 MB/s eta 0:00:05     |██████████████████████████▉     | 383.2 MB 21.5 MB/s eta 0:00:04     |██████████████████████████████▉ | 440.4 MB 17.3 MB/s eta 0:00:01     |███████████████████████████████▋| 450.9 MB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 31.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.1.0)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tb-nightly~=2.6.0.a\n",
      "  Downloading tb_nightly-2.6.0a20210430-py3-none-any.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 24.8 MB/s eta 0:00:01     |████████████████████████████▏   | 5.2 MB 24.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-nightly~=2.6.0.dev\n",
      "  Downloading keras_nightly-2.6.0.dev2021043000-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (3.11.4)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (0.2.0)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tf-nightly) (1.12.1)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tf-estimator-nightly~=2.5.0.dev\n",
      "  Downloading tf_estimator_nightly-2.5.0.dev2021032601-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 33.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Using cached numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting cached-property; python_version < \"3.8\"\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (3.2.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.23.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.14.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tb-nightly~=2.6.0.a->tf-nightly) (46.4.0.post20200518)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (1.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2020.4.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.4.8)\n",
      "\u001b[31mERROR: jupyterlab 1.2.5 has requirement jupyterlab_server~=1.0.0, but you'll have jupyterlab-server 1.1.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, cached-property, h5py, wheel, six, keras-preprocessing, gast, absl-py, opt-einsum, tensorboard-plugin-wit, tensorboard-data-server, grpcio, tb-nightly, keras-nightly, typing-extensions, flatbuffers, astunparse, tf-estimator-nightly, tf-nightly\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.4\n",
      "    Uninstalling numpy-1.18.4:\n",
      "      Successfully uninstalled numpy-1.18.4\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.14.0\n",
      "    Uninstalling six-1.14.0:\n",
      "      Successfully uninstalled six-1.14.0\n",
      "  Attempting uninstall: keras-preprocessing\n",
      "    Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.9.0\n",
      "    Uninstalling absl-py-0.9.0:\n",
      "      Successfully uninstalled absl-py-0.9.0\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt-einsum 0+untagged.56.g2664021.dirty\n",
      "    Uninstalling opt-einsum-0+untagged.56.g2664021.dirty:\n",
      "      Successfully uninstalled opt-einsum-0+untagged.56.g2664021.dirty\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.27.2\n",
      "    Uninstalling grpcio-1.27.2:\n",
      "      Successfully uninstalled grpcio-1.27.2\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cached-property-1.5.2 flatbuffers-1.12 gast-0.4.0 grpcio-1.37.1 h5py-3.1.0 keras-nightly-2.6.0.dev2021043000 keras-preprocessing-1.1.2 numpy-1.19.5 opt-einsum-3.3.0 six-1.15.0 tb-nightly-2.6.0a20210430 tensorboard-data-server-0.6.0 tensorboard-plugin-wit-1.8.0 tf-estimator-nightly-2.5.0.dev2021032601 tf-nightly-2.6.0.dev20210430 typing-extensions-3.7.4.3 wheel-0.36.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow\n",
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "assert float(tf.__version__[:3]) >= 2.3\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0-dev20210430'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quantization\n",
    "### 1-1. [Post-training quantization](https://blog.tensorflow.org/2019/06/tensorflow-integer-quantization.html)\n",
    "\n",
    "post-training quantization은 학습을 한 다음에 quantization을 하는 방법이다. quantization aware training보다는 accuracy가 좀 떨어질 수 있지만, 학습된 모델을 가져와서 쉽게 쓸 수 있다. tensorflow에서 다음과 같은 방법들을 제공하고 있다.\n",
    "\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114367164-2232b100-9bb7-11eb-81e7-b9b8dec09de5.png\" width=\"700\" height=\"700\"/>  \n",
    "- dynamic range quantization: weight은 8 bit integer로 저장. inference 시에는 8bit float로 바뀜. activation은 range에 따라서(dynamic) 8 bit float으로 저장\n",
    "\n",
    "\n",
    "나중에 Edge TPU가 있는 Coral board를 사용할 것이기 때문에, post-training integer quantization을 다뤄 볼 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 quantizaion을 할 간단한 모델을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4582 - accuracy: 0.8366 - val_loss: 0.3499 - val_accuracy: 0.8768\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.3166 - accuracy: 0.8874 - val_loss: 0.3116 - val_accuracy: 0.8892\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2839 - accuracy: 0.8991 - val_loss: 0.2908 - val_accuracy: 0.8947\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.2601 - accuracy: 0.9058 - val_loss: 0.2794 - val_accuracy: 0.8987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ac81d29d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the input image\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,  \n",
    "  epochs=4,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델을 변환하기 위해서는 [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert)라는 것을 이용해야 합니다. 이것은 TensorFlow 모델을 TensorFlow Lite 모델로 바꿔 줍니다.\n",
    "\n",
    "다음과 같이 tflite 모델로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpd7dien96/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# 이 사이에 많은 옵션 추가 가능\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환할 때 아무런 옵션을 주지 않았으므로 이 모델은 단순히 32-bit float tflite 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114906428-f9c7e280-9e54-11eb-9038-9854d5c8d741.png\" width=\"700\" height=\"700\"/>  \n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114907026-9e4a2480-9e55-11eb-980f-55999688c260.png\" width=\"500\" height=\"500\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 셀을 실행하면 quantization이 완료 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1mnhdxwp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1mnhdxwp/assets\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# 옵션들\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen # 함수로 설정해야 한다.\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8] # 이 모델은 integer 연산만 할 것\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "# 옵션 끝\n",
    "\n",
    "post_quant_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate을 위한 helper function이다. input type을 integer로 했으므로, quantize를 한 다음에 inference를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter):\n",
    "    global test_images\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if (i+1) % 1000 == 0:\n",
    "              print('Evaluated on {n} results so far.'.format(n=i+1))\n",
    "\n",
    "        # quantize input\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point # test image를 integer로 변환\n",
    "\n",
    "        # Pre-processing: add batch dimension\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Post-processing: remove batch dimension and find the digit with highest\n",
    "        # probability.\n",
    "        output = interpreter.tensor(output_index)\n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "        \n",
    "    print()\n",
    "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tflite 모델로 inference를 하려면 [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter)를 이용한다. `model_content` 대신에 `model_path`를 이용하면 tflite 파일을 불러올 수도 있다.  \n",
    "inference를 하기 위해서는  \n",
    "interpreter 생성 &#8594; `allocate_tensor` &#8594; `set_tensor` &#8594; `invoke` 순으로 실행한다.  \n",
    "지금 코드는 inference 할 때 image를 하나씩 처리하는데, batch 단위로 처리하려면 `resize_tensor_input`을 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n",
      "\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 32bit float tflite 모델로 interpreter 만듦\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "# inference 할 때 input batch 수를 설정하는 법\n",
    "# input_details = interpreter.get_input_details()[0]\n",
    "# interpreter.resize_tensor_input(input_details[\"index\"], [10, 28, 28]) # <-- 추가\n",
    "\n",
    "interpreter.allocate_tensors()\n",
    "tflite_test_accuracy, _ = evaluate_model(interpreter)\n",
    "\n",
    "# 8bit integer tflite\n",
    "post_quant_interpreter = tf.lite.Interpreter(model_content=post_quant_tflite_model)\n",
    "post_quant_interpreter.allocate_tensors()\n",
    "post_quant_tflite_test_accuracy, output = evaluate_model(post_quant_interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_input_details`, `get_output_details`를 통해 input과 output의 type 및 quantization 정보를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'input_1',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 28, 28], dtype=int32),\n",
       "  'shape_signature': array([-1, 28, 28], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.003921568859368563, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Identity',\n",
       "  'index': 24,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.00390625, 0),\n",
       "  'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "   'zero_points': array([0], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_quant_interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 inference output이 정수임을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   1,   0,   0, 163,   0,  65,  26,   1]], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "float 32 모델과 accuracy를 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite test accuracy: 0.8987\n",
      "Post-training Quant TFLite test_accuracy: 0.8962\n"
     ]
    }
   ],
   "source": [
    "print('TFLite test accuracy:', tflite_test_accuracy)\n",
    "print('Post-training Quant TFLite test_accuracy:', post_quant_tflite_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 파일로 저장해서 실제로 모델 크기가 줄어들었는지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.08170700073242188\n",
      "Quantized model in Mb: 0.02512359619140625\n"
     ]
    }
   ],
   "source": [
    "# create temp file\n",
    "_, tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(tflite_file, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "with open(post_quant_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_tflite_model)\n",
    "\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(tflite_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(post_quant_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습: TensorFlow에서 제공하는 pretrain model 가져와서 post-training quantization 해보기\n",
    "\n",
    "TensorFlow에서는 여러 pretrained model을 제공한다. 아무거나 하나 가져와서 post-training quantization을 해보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain 모델 import 하기.\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 test image를 이용할 것이다. quantization을 할 때, representative data로는 이 이미지 하나만 이용해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F8AA81943D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://cdn.education.com/files/526001_527000/526114/file_526114.jpg\"\n",
    "os.system(\"curl \" + url + \" > balloon.jpg\")\n",
    "img_path = 'balloon.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "display(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x) # 각 모델을 학습할 때 preprocess했던 방법을 사용 - ResNet의 경우 zero centering etc 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate 할 때는 다음 함수를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_practice(interpreter, test_images):\n",
    "    \n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    input_index = input_details[\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "    # Run predictions on every image in the \"test\" dataset.\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images):\n",
    "        if i % 1000 == 0:\n",
    "            print('Evaluated on {n} results so far.'.format(n=(i+1)))\n",
    "\n",
    "        # quantize input\n",
    "        if input_details['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "            test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "        # Pre-processing: add batch dimension\n",
    "        test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "        # Run inference.\n",
    "        interpreter.invoke()\n",
    "\n",
    "        preds = interpreter.tensor(output_index)\n",
    "        preds = np.expand_dims(preds()[0], axis=0)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02782093', 'balloon', 0.94828665), ('n04023962', 'punching_bag', 0.034170937), ('n03942813', 'ping-pong_ball', 0.0027104064)]\n"
     ]
    }
   ],
   "source": [
    "# 1. import한 pretrain 모델 가져오기\n",
    "imagenet = ResNet50(weights='imagenet')\n",
    "\n",
    "preds = imagenet.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqpxsu64q/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqpxsu64q/assets\n"
     ]
    }
   ],
   "source": [
    "# 크기 비교를 위한 32bit float tflite 모델 생성\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet)\n",
    "imagenet_tflite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa6v1x9sy/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpa6v1x9sy/assets\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "source": [
    "# 2. full integer post-training quantization\n",
    "import numpy as np\n",
    "\n",
    "def representative_data_gen():\n",
    "    yield [x] # test 할 이미지 1개만 사용\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(imagenet)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# ensure that if any ops can't be quantized, the converter throws an error.\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "post_quant_imagenet_tflite = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1 results so far.\n",
      "Predicted: [('n02782093', 'balloon', 240), ('n04023962', 'punching_bag', 11), ('n03942813', 'ping-pong_ball', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 3. make interpreter and do inference\n",
    "# 32bit float tflite 모델로 interpreter 만듦\n",
    "post_quant_imagenet_interpreter = tf.lite.Interpreter(model_content=post_quant_imagenet_tflite)\n",
    "post_quant_imagenet_interpreter.allocate_tensors()\n",
    "\n",
    "preds = evaluate_model_practice(post_quant_imagenet_interpreter, x)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크기 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 97.43085861206055\n",
      "Quantized model in Mb: 25.079750061035156\n"
     ]
    }
   ],
   "source": [
    "# measure sizes of models.\n",
    "_, imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
    "_, post_quant_imagenet_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "\n",
    "with open(imagenet_tflite_file, 'wb') as f:\n",
    "    f.write(imagenet_tflite)\n",
    "\n",
    "with open(post_quant_imagenet_tflite_file, 'wb') as f:\n",
    "    f.write(post_quant_imagenet_tflite)\n",
    "\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(imagenet_tflite_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(post_quant_imagenet_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. [Quantization Aware Training(QAT)](https://blog.tensorflow.org/2020/04/quantization-aware-training-with-tensorflow-model-optimization-toolkit.html)\n",
    "\n",
    "수업 시간에 배웠 듯이 QAT는 나중에 quantization을 할 것이라는 것을 가정하고 학습하는 것이다. scratch부터 학습해도 되지만, 이미 학습된 모델을 fine tuning 하는 것이 좋다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize whole model\n",
    "\n",
    "Edge TPU와 같이 fully quantized 모델이 필요한 경우에는 전체 모델에 대해 QAT를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 학습한 float32모델로 QAT를 해볼 것이다.`quantize_model`함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 28, 28)            3         \n",
      "_________________________________________________________________\n",
      "quant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 28, 28, 16)        195       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 14, 14, 16)        1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_1 (QuantizeWrap (None, 14, 14, 32)        4707      \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d_1 (Quant (None, 7, 7, 32)          1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 1568)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                15695     \n",
      "=================================================================\n",
      "Total params: 20,604\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "quant_aware_model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer에 quant가 붙은 것을 확인할 수 있다. 그 다음으로는, data의 일부를 이용해서 fine tuning 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 435ms/step - loss: 0.2096 - accuracy: 0.9256 - val_loss: 0.2689 - val_accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a086584d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data 일부를 이용해서 fine tuning\n",
    "train_images_subset = train_images[0:1000] # out of 60000\n",
    "train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "quant_aware_model.fit(train_images_subset, train_labels_subset, # qat 진행\n",
    "                  batch_size=500, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 quantization을 하지 않았으므로 weight은 실수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-7.7812816e-03, -1.2126947e-02,  9.7242072e-03,  2.4985732e-03,\n",
       "         2.2446243e-02, -7.8753596e-03,  6.2441267e-03, -4.4326237e-03,\n",
       "         2.1035101e-02, -2.2471838e-02,  2.1389256e-01,  1.0915109e-02,\n",
       "        -1.6672490e-04, -7.3085800e-03, -1.1691596e-02,  1.5000826e-02],\n",
       "       dtype=float32),\n",
       " array([[[[-0.00525647, -0.29738104, -0.29413444,  0.21692048,\n",
       "           -0.15882193, -0.15578982, -0.22276993,  0.04558412,\n",
       "           -0.23259796,  0.17230383,  0.11304303, -0.2827086 ,\n",
       "            0.16056079, -0.02098618,  0.04581424,  0.08820706]],\n",
       " \n",
       "         [[ 0.08438446, -0.07145761,  0.16581048,  0.3423589 ,\n",
       "           -0.5456619 ,  0.24957405,  0.1933133 ,  0.09705455,\n",
       "            0.43871638,  0.38245338,  0.26191664, -0.14987879,\n",
       "            0.22836232, -0.08126036, -0.12456531, -0.35000733]],\n",
       " \n",
       "         [[-0.1544532 ,  0.20232083,  0.1441259 ,  0.10859156,\n",
       "           -0.14982082,  0.08728001,  0.16210154,  0.26284206,\n",
       "           -0.1922192 ,  0.0329748 ,  0.2680842 ,  0.42842725,\n",
       "           -0.38080233,  0.17801876,  0.28138265,  0.29804915]]],\n",
       " \n",
       " \n",
       "        [[[-0.1582107 ,  0.09613181, -0.5048268 ,  0.24985127,\n",
       "            0.37065208,  0.26322165, -0.34492183,  0.18710293,\n",
       "            0.20470527, -0.40226802, -0.37411144,  0.19204895,\n",
       "            0.27673772, -0.19850102, -0.1490808 , -0.29151714]],\n",
       " \n",
       "         [[ 0.36527303,  0.2156609 ,  0.38057873,  0.06102251,\n",
       "            0.28951573,  0.19543791,  0.12548098,  0.22296788,\n",
       "            0.07518728, -0.09505427, -1.2708972 , -0.07751624,\n",
       "           -0.14752652, -0.28462663, -0.28263596, -0.36421502]],\n",
       " \n",
       "         [[-0.22035713,  0.09060574,  0.13654274,  0.03575949,\n",
       "           -0.06120847,  0.1875885 ,  0.11493152,  0.11704226,\n",
       "           -0.2667191 ,  0.33843783, -0.8371422 , -0.06468181,\n",
       "            0.0445127 ,  0.5067233 ,  0.39856195,  0.14075796]]],\n",
       " \n",
       " \n",
       "        [[[-0.16279365,  0.08817385, -0.17565274, -0.23672494,\n",
       "           -0.0827985 , -0.1233878 , -0.37654167,  0.09338825,\n",
       "            0.27840737, -0.22483297, -0.6934184 ,  0.30856943,\n",
       "            0.3370996 , -0.22250685,  0.09289547,  0.16944973]],\n",
       " \n",
       "         [[ 0.1927163 , -0.00398055,  0.22361466, -0.1943875 ,\n",
       "            0.10250862, -0.12501189,  0.32618093, -0.09477677,\n",
       "           -0.04250655, -0.3308883 , -1.0385522 , -0.00127572,\n",
       "           -0.05985283,  0.03393939, -0.10234493,  0.23866259]],\n",
       " \n",
       "         [[ 0.10727944,  0.02349485, -0.04047004, -0.27295592,\n",
       "            0.18919122, -0.16521738,  0.03725553, -0.15933093,\n",
       "           -0.25125518,  0.22096033, -0.38258478, -0.34284732,\n",
       "           -0.12600888,  0.16729055, -0.11196968,  0.07918269]]]],\n",
       "       dtype=float32),\n",
       " -1,\n",
       " array([-0.36496592, -0.29784554, -0.50515145, -0.34161928, -0.5457177 ,\n",
       "        -0.26225695, -0.37657073, -0.26184094, -0.4382082 , -0.40266022,\n",
       "        -1.2708087 , -0.42783752, -0.38103083, -0.5067301 , -0.3981802 ,\n",
       "        -0.36327004], dtype=float32),\n",
       " array([0.36496592, 0.29784554, 0.50515145, 0.34161928, 0.5457177 ,\n",
       "        0.26225695, 0.37657073, 0.26184094, 0.4382082 , 0.40266022,\n",
       "        1.2708087 , 0.42783752, 0.38103083, 0.5067301 , 0.3981802 ,\n",
       "        0.36327004], dtype=float32),\n",
       " -5.988006,\n",
       " 5.990133]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실수 weight이므로 QAT를 한다고 해서 accuracy가 떨어지지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8986999988555908\n",
      "QAT test accuracy: 0.8996999859809875\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=0)\n",
    "\n",
    "_, quant_aware_model_accuracy = quant_aware_model.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('QAT test accuracy:', quant_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 QAT로 학습한 모델을 quantization 해보자. post-training quantization과 비슷하게 TFLiteConverter를 이용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as reshape_layer_call_and_return_conditional_losses, reshape_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, conv2d_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses while saving (showing 5 of 25). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm59v_ce6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm59v_ce6/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8 # or tf.int8\n",
    "converter.inference_output_type = tf.uint8 # or tf.int8\n",
    "\n",
    "\n",
    "QAT_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantization을 해도 accuracy가 떨어지지 않는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n",
      "\n",
      "Baseline test accuracy: 0.8986999988555908\n",
      "Post-training Quantized test accuracy: 0.8962\n",
      "Quantized QAT test accuracy: 0.9003\n"
     ]
    }
   ],
   "source": [
    "QAT_interpreter = tf.lite.Interpreter(model_content=QAT_tflite_model)\n",
    "QAT_interpreter.allocate_tensors()\n",
    "\n",
    "QAT_test_accuracy, output = evaluate_model(QAT_interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Post-training Quantized test accuracy:', post_quant_tflite_test_accuracy)\n",
    "print('Quantized QAT test accuracy:', QAT_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 크기가 1/4로 줄어든 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float tflite model in Mb: 0.08170700073242188\n",
      "Quantized QAT model in Mb: 0.0255889892578125\n"
     ]
    }
   ],
   "source": [
    "# create temp file\n",
    "_, QAT_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(QAT_tflite_file, 'wb') as f:\n",
    "    f.write(QAT_tflite_model)\n",
    "\n",
    "print(\"Float tflite model in Mb:\", os.path.getsize(tflite_file) / float(2**20))\n",
    "print(\"Quantized QAT model in Mb:\", os.path.getsize(QAT_tflite_file) / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize some layers\n",
    "\n",
    "Edge TPU 등 특별한 하드웨어가 아닌 경우에는 선택적으로 layer를 quantization할 수도 있다.\n",
    "\n",
    "**Tips for better accuracy**\n",
    "* from scratch보다는 fine tuning\n",
    "* 뒤쪽 layer를 quantization 하기\n",
    "* 특별히 중요한 layer는 quantization 피하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`quantize_annotate_layer` 함수를 이용해서 특정 layer를 quantization할 것이라고 표시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def apply_quantization_to_dense(layer): # layer를 input으로 받아서 어떤 layer에만 quant 할거라고 표시하는 것\n",
    "    # if isinstance(layer, tf.keras.layers.Dense)\n",
    "    if layer.name == 'dense':\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer) # quant\n",
    "    else:\n",
    "        return layer # 그대로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clone_model` 함수와 helper function을 이용해서 annotated model을 만들고, `quantize_apply`함수를 이용해서 QAT를 위한 모델을 만들면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                15695     \n",
      "=================================================================\n",
      "Total params: 20,495\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 5\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 fine tuning이 아니라 처음부터 모델을 만드는 것이면 다음과 같이 모델을 만들면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 12)        120       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "quant_dense_1 (QuantizeWrapp (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,415\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 5\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "annotated_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Dense(10)) # 직접 설정\n",
    "])\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "quant_conv2d_3 (QuantizeWrap (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2028)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                20290     \n",
      "=================================================================\n",
      "Total params: 20,437\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 27\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(inputs)\n",
    "x = tfmot.quantization.keras.quantize_annotate_layer(tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'))(x) # 직접 설정\n",
    "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "outputs = tf.keras.layers.Dense(10)(x)\n",
    "\n",
    "annotated_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [Pruning](https://blog.tensorflow.org/2019/05/tf-model-optimization-toolkit-pruning-API.html)\n",
    "\n",
    "pruning은 불필요한(0에 가까운) weight을 0으로 만들어 없애면서 optimize를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfmot.sparsity.keras.prune_low_magnitude`을 이용해서 모델을 만든다. 이 때, pruning 방식인 `tfmot.sparsity.keras.PolynomialDecay` 을 parameter로 넘겨준다. 다음과 같은 hyperparemeter가 있다.\n",
    "* `initial_sparsity`: pruning을 시작할 때의 sparsity를 몇으로 할 지\n",
    "* `final_sparsity`: pruning을 끝낼 때의 sparsity를 몇으로 할 지\n",
    "* `begin_step`: pruning을 언제부터 진행할 지 (batch 단위의 step)\n",
    "* `end_step`: pruning을 언제 끝낼 지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruning을 하기 전에 baseline model을 저장하겠다. (크기 비교시 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline model to: /tmp/tmp6nawnhkh.h5\n"
     ]
    }
   ],
   "source": [
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2193: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_reshape  (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d ( (None, 28, 28, 16)        306       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 14, 14, 16)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d_1 (None, 14, 14, 32)        9250      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 7, 7, 32)          1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten  (None, 1568)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 10)                31372     \n",
      "=================================================================\n",
      "Total params: 40,932\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 20,442\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "\n",
    "num_images = train_images.shape[0]\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs # 총 step 개수\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit`을 할 때, `tfmot.sparsity.keras.UpdatePruningStep`을 callback으로 불러야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5050: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5050: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 13s 23ms/step - loss: 0.2850 - accuracy: 0.8995\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.2993 - accuracy: 0.8931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a48467e10>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(), # 필요!\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline 모델과 accuracy를 비교해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8986999988555908\n",
      "Pruned test accuracy: 0.8884999752044678\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruned model의 크기가 줄어들었는 지 확인해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에 summary를 보면 pruning을 하기 위해 non-trainable parameter가 생긴 것을 알 수 있다. `tfmot.sparsity.keras.strip_pruning`을 이용해서 pruning에 사용한 variable을 제거해 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: /tmp/tmpvbhaq1f9.h5\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning) # pruning 시 늘어난 non-trainable parameter를 없애기 위해\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tflite 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5bh73zqf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp5bh73zqf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: /tmp/tmppm_i1v8y.tflite\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pruning을 해도 weight matrix의 크기는 그대로이다. 하지만 matrix의 값이 대부분 0이기 때문에 감소된 크기를 확인하려면 실제로 zip파일 등으로 압축하는 과정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped baseline Keras model: 78962.00 bytes\n",
      "Size of gzipped pruned Keras model: 26548.00 bytes\n",
      "Size of gzipped pruned TFlite model: 25355.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine pruning and quantization\n",
    "pruning을 한 다음에 post-training quantization까지 하면 크기를 더 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkec0cvbb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkec0cvbb/assets\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: /tmp/tmptxymmuxl.tflite\n",
      "Size of gzipped baseline Keras model: 78962.00 bytes\n",
      "Size of gzipped pruned and quantized TFlite model: 9494.00 bytes\n"
     ]
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "    f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline 과 accuracy를 비교해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "Evaluated on 10000 results so far.\n",
      "\n",
      "Baseline test accuracy: 0.8986999988555908\n",
      "Pruned and quantized TFLite test_accuracy: 0.8892\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy, _ = evaluate_model(interpreter)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QAT와 비슷하게, pruning도 layer를 선택적으로 pruning 할 수 있다. 이 부분은 [document](https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide)를 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ONNX](https://github.com/onnx/onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 tensorflow 모델을 이용해서 tflite 모델을 만들고 싶은데, tensorflow 모델은 없고 pytorch 모델만 있다면...? 이럴 때 사용하는 것이 ONNX이다. ONNX는 서로다른 deep learning framework에서 모델을 사용하고 싶을 때, 모델을 원하는 framework로 변환해 주는 기능을 제공한다. \n",
    "  \n",
    "즉, ONNX를 이용해서 pytorch 모델을 tensorflow 모델로 바꾼 다음, tensorflow lite 모델을 만들어 주면 된다.\n",
    "예시 코드를 ONNX.ipynb로 올려 두었다.\n",
    "  \n",
    "* 서치를 해보니 모든 모델을 완벽하게 바꿔주는 것은 아니여서 (accuracy 감소 및 변환이 안되는 operation 등이 있을 수 있다), 처음부터 학습해야 하는 경우에는 pytorch 코드를 tensorflow 코드로 구현해서 모델을 학습하는게 좋고, 그게 잘 안되거나 pretrained pytorch 모델을 꼭 써야하는 경우에 이용하면 좋을 것 같다.\n",
    "* 또한 pytorch 버전, tensorflow 버전, onnx 버전 등 버전에 영향을 많이 받는 것 같아서 실제로 본인이 원하는 모델을 바꾸려면 많은 시행착오가 필요해 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "### Due: 21.05.03 15:29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 과제는 8-bit integer quantization을 했을 때, 어떤 식으로 계산이 이루어지는 지 source code 분석을 통해 알아보는 과제이다.\n",
    "\n",
    "먼저, TFLite에서 지원하는 operation에 대한 부분은 다음 파일에 있다.\n",
    "* [register.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/register.cc)\n",
    "\n",
    "ReLU를 예시로 들면, ReLU는 `Register_RELU()` 함수가 있다.  \n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114957599-526e9e00-9e9c-11eb-928a-17a9dda8b050.PNG\" width=\"500\" height=\"500\"/> \n",
    "\n",
    "이 함수는 다음 파일에 구현이 되어있다.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/37704174/114957607-54d0f800-9e9c-11eb-9be3-1aa960f0d925.PNG\" width=\"500\" height=\"500\"/>  \n",
    "\n",
    "* [activations.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc)\n",
    "\n",
    "4가지 함수가 있다.\n",
    "* Init: new parameter allocation\n",
    "* Free: free allocated parameters\n",
    "* Prepare: fill the parameter values\n",
    "* **Eval**: Run computation of operation using filled parameters\n",
    "\n",
    "즉, 어떻게 계산이 이루어 지는 지를 보려면 `ReluEval`을 보면 될 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "보고서에 포함되어야 하는 내용은 다음과 같다.\n",
    "1. float min, float max로부터 quantization parameter (scale, zero point)가 어떻게 계산 되는 지\n",
    "  * [quantization_utils.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantization_utils.cc)\n",
    "    * `GetAsymmetricQuantizationParams`, `GetSymmetricQuantizationParams`\n",
    "2. 3가지 layer에 대해 quantization parameter를 이용해서 어떤 식으로 integer-only arithmetic을 수행하는 지\n",
    "    - **ReLU**, **logistic**\n",
    "      - [activations.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/activations.cc), [reference_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/reference_ops.h), [optimized_ops.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h)\n",
    "    - **Fully connected**\n",
    "      - [fully_connected.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/fully_connected.cc), [fully_connected.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/reference/integer_ops/fully_connected.h)\n",
    " \n",
    "\n",
    "(함수의 정확한 동작, c/cpp의 정확한 문법을 알 필요는 없고, quantization과 computation에 대한 전반적인 흐름을 분석하면 된다.(어떤 함수를 부르게 되고, 어떤 정수 연산을 하게 되는지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제출\n",
    "pdf로 변환 후 **학번_이름_HW5.pdf** (ex. 2020_12345_김우중_HW5.pdf) etl에 제출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
